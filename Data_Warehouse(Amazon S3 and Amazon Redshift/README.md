### This project —Åonsist from the following parts:
1. Creating staging and analytics tables
2. Load data from S3 in Redshift, 
3. Transforms data into a set of fact and dimensional tables 
4. Test ETL by running the analytic queries on your Redshift database to compare your results

### Sparkify 
***Sparkify*** - database is designed to store information about user activity of a new music streaming service.
This information will help the analyst to make accurate recommendations and musical suggestions to his users, 
thereby increasing their loyalty.

### Project steps
1. In the first step we can create two staging tables ***staging_song***, ***staging_event*** (with all fields in json files) for song and event datasets and five tables which will form the data warehouse:
***Fact Table***
**songplays** - records in event data associated with song plays i.e. records with page NextSong
    (songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent)
***Dimension Tables***
**users** - users in the app
    (user_id, first_name, last_name, gender, level)
**songs** - songs in music database
    (song_id, title, artist_id, year, duration)
**artists** - artists in music database
    (artist_id, name, location, lattitude, longitude)
**time** - timestamps of records in songplays broken down into specific units
    (start_time, hour, day, week, month, year, weekday)

In order to create tables for a database using the python script create_tables.py from project directory with help following commands:
*python3 create_tables.py*

2. Initially, Amazon S3 contained the following Json files in the following way:
* Song data: s3://udacity-dend/song_data, The first dataset is a subset of real data from the Million Song Dataset. Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID. For example, here are filepaths to two files in this dataset.
* Log data: s3://udacity-dend/log_data, The second dataset consists of log files in JSON format generated by this event simulator based on the songs in the dataset above. These simulate app activity logs from an imaginary music streaming app based on configuration settings. 

In the next step after creating tables were loaded into staging tables ***staging_song***, ***staging_event*** from Amazon S3 (***Amazon Simple Storage Service***) is a service offered by Amazon Web Services (AWS) that provides object cloud storage) to Redshift creating analytics tables (***Amazon Redshift*** is an Internet hosting service and data warehouse product which forms part of the larger cloud-computing platform Amazon Web Services, which built on top of technology from the massive parallel processing to handle large scale data sets and database migrations)

3. Relational database model (analytics tables) was selected with the organization of tables in star schema, because this scheme is convenient for storing multidimensional indicators. Scheme consists of one fact table and four dimension table. A detailed structure of the tables can be seen below:

!](images/img.png "Structure of DB")

You can start of creating tables and ETL process to load the info from json to the staging tables and create data warehouse in stars scheme with help following commands:
*python3 etl.py*

4. Create some simple query to our database
* 
*python3 test.py*


### Project template
* **data** - The first dataset (**Song Dataset**) is a subset of real data from the Million Song Dataset. Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID. For example, here are filepaths to two files in this dataset.The second dataset (**Log dataset**) consists of log files in JSON format generated by this event simulator based on the songs in the dataset above. These simulate app activity logs from a music streaming app based on specified configurations.The log files in the dataset you'll be working with are partitioned by year and month. For example, here are filepaths to two files in this dataset.
* **test.ipynb** - displays the first few rows of each table to let you check your database.
* **create_tables.py** - drops and creates your tables. You run this file to reset your tables before each time you run your ETL scripts.
* **etl.py** - reads and processes files from song_data and log_data and loads them into your tables. You can fill this out based on your work in the ETL notebook.
* **README.md** -  provides discussion on your project.
* **sql_queries.py** - contains all your sql queries, and is imported into the last three files above.
* **std_load_error.py** - file that helps to disclose errors with the database


### Discussion
***Benefits Amazon Redshift

IMPROVED PERFORMANCE

Amazon Redshift performance is ten times higher than other data warehouses. This service uses machine learning, a massively parallel architecture, hardware optimized for computing, and caching of the resulting datasets to ensure high throughput and response time at the fraction of a second, even with thousands of parallel queries. Using Redshift, you can spend less time waiting and more time working with analytic information based on data.
SIMPLE ADJUSTMENT, DEPLOYMENT AND MANAGEMENT

Amazon Redshift is easy to use and allows you to deploy a new data store in minutes. Redshift automates most of the standard administrative tasks associated with monitoring and scaling data storage, as well as managing it. This allows you to get rid of complex procedures for managing local data stores.

Amazon S3 is great for creating a data warehouse for a Sparkify application.


