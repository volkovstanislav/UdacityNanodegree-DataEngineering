### This project —Åonsist from the following parts:
1. Reads data from S3, 
2. Processes data using Spark
3. Test ETL by running the analytic queries using Spark to compare results
4. Writes data back to S3 (or localy)

### Sparkify 
***Sparkify*** - database is designed to store information about user activity of a new music streaming service.
This information will help the analyst to make accurate recommendations and musical suggestions to his users, 
thereby increasing their loyalty.

### Project steps
1. Reads data from S3 using Spark
Here are the S3 links for each:
Song data located following link: *s3://udacity-dend/song_data*
Log data located following link: *s3://udacity-dend/log_data*

**Song Dataset**
The first dataset is a subset of real data from the Million Song Dataset. Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID. For example, here are filepaths to two files in this dataset.
* song_data/A/B/C/TRABCEI128F424C983.json
* song_data/A/A/B/TRAABJL12903CDCF1A.json

And below is an example of what a single song file, TRAABJL12903CDCF1A.json, looks like.

{<font color="red">"num_songs"</font>: 1, <font color="red">"artist_id"</font>: "ARJIE2Y1187B994AB7", <font color="red">"artist_latitude"</font>: null, <font color="red">"artist_longitude"</font>: null, <font color="red">"artist_location"</font>: "", <font color="red">"artist_name"</font>: "Line Renaud", <font color="red">"song_id"</font>: "SOUPIRU12A6D4FA1E1", <font color="red">"title"</font>: "Der Kleine Dompfaff", <font color="red">"duration"</font>: 152.92036, <font color="red">"year"</font>: 0}

**Log Dataset**
The second dataset consists of log files in JSON format generated by this event simulator based on the songs in the dataset above. These simulate app activity logs from an imaginary music streaming app based on configuration settings.

The log files in the dataset you'll be working with are partitioned by year and month. For example, here are filepaths to two files in this dataset.

* log_data/2018/11/2018-11-12-events.json
* log_data/2018/11/2018-11-13-events.json

And below is an example of what the data in a log file, 2018-11-12-events.json, looks like.
!](images/log-data.png "2018-11-12-events.json")

2. On the second step we processed data using Spark, create five Dataframes:

**songplays_table** - records in event data associated with song plays i.e. records with page NextSong
    (songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent)
    
***Dimension Tables***
**users_table** - users in the app
    (user_id, first_name, last_name, gender, level)
**songs_table** - songs in music database
    (song_id, title, artist_id, year, duration)
**artists_table** - artists in music database
    (artist_id, name, location, lattitude, longitude)
**time_table** - timestamps of records in songplays broken down into specific units
    (start_time, hour, day, week, month, year, weekday)

3. On the third step we test dataframes using Spark 

4. On the fourth step we saved dataframes in parquet format

You can start of ETL processes to load the info from json to the staging tables and create data warehouse in stars scheme with help following commands:
*python3 etl.py*



### Project template
* **etl.py** - reads and processes files from song_data and log_data and loads them into your tables. You can fill this out based on your work in the ETL notebook.
* **README.md** - provides discussion on your project.



