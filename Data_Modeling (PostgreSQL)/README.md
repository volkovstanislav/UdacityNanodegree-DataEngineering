### Sparkify 
*Sparkify* - database is designed to store information about user activity of a new music streaming service.
This information will help the analyst to make accurate recommendations and musical suggestions to his users, 
thereby increasing their loyalty.

### Database description
In this project i create database with help Postgres and ETL pipeline with tables designed to optimize queries on song play analysis. Usually the key rule of data separation is the following: the fact table contains what is analyzed, and the dimension table contains what the analysis is about. In our application, the facts are the music listened by users, and the measurement table is information about all the objects of the application: users, music, and performers.
As a result, a relational database model was selected with the organization of tables in star schema, because this scheme is convenient for storing multidimensional indicators. Scheme consists of one fact table and four dimension table. A detailed structure of the tables can be seen below:

!](images/img.png "Structure of DB")

The data on client logs and application content is located in **JSON**, so using Python and the library json, all data were sparsened into relational database tables, as it is more convenient and more flexible to work with databases than to constantly read more log files, which is a laborious task.

The following tasks were completed:
1. Designed relational database with star schema
2. The json was read and translated into relational database data into dimension and fact tables.
3. Using queries in the SQL the data was tested for the correct upload

### Script startup process
* In order to create tables for a database using a script create_tables.py from project directory with help following commands:
*python3 create_tables.py*
* After that you can start the ETL process to load the info from json to the base tables with help following commands:
*python3 etl.py*

* Correctness of creating tables can be checked using a jupyther notebook test.ipynb following way:
1. The sql extension is already loaded: *%load_ext sql;*
2. Connected to Database: *%sql postgresql://student:student@127.0.0.1/sparkifydb;*
3. Write queries in SQL to the created table: *%sql SELECT * FROM songplays;*

### Project template
* **data** - The first dataset (**Song Dataset**) is a subset of real data from the Million Song Dataset. Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID. For example, here are filepaths to two files in this dataset.The second dataset (**Log dataset**) consists of log files in JSON format generated by this event simulator based on the songs in the dataset above. These simulate app activity logs from a music streaming app based on specified configurations.The log files in the dataset you'll be working with are partitioned by year and month. For example, here are filepaths to two files in this dataset.
* **images** - pictures for script and documentation
* **etl.ipynb** - reads and processes a single file from song_data and log_data and loads the data into your tables. This notebook contains detailed instructions on the ETL process for each of the tables.
* **test.ipynb** - displays the first few rows of each table to let you check your database.
* **create_tables.py** - drops and creates your tables. You run this file to reset your tables before each time you run your ETL scripts.
* **etl** - reads and processes files from song_data and log_data and loads them into your tables. You can fill this out based on your work in the ETL notebook.
* **README.md** -  provides discussion on your project.
* **sql_queries.py** - contains all your sql queries, and is imported into the last three files above.





